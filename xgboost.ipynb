{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dalex as dx\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = dx.datasets.load_titanic()\n",
    "X = titanic.drop(columns='survived')\n",
    "y = titanic.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "      <th>embarked</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age class     embarked   fare  sibsp  parch\n",
       "0    male  42.0   3rd  Southampton   7.11      0      0\n",
       "1    male  13.0   3rd  Southampton  20.05      0      2\n",
       "2    male  16.0   3rd  Southampton  20.05      1      1\n",
       "3  female  39.0   3rd  Southampton  20.05      1      1\n",
       "4  female  16.0   3rd  Southampton   7.13      0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>class_2nd</th>\n",
       "      <th>class_3rd</th>\n",
       "      <th>class_deck crew</th>\n",
       "      <th>class_engineering crew</th>\n",
       "      <th>class_restaurant staff</th>\n",
       "      <th>class_victualling crew</th>\n",
       "      <th>embarked_Cherbourg</th>\n",
       "      <th>embarked_Queenstown</th>\n",
       "      <th>embarked_Southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   fare  sibsp  parch  gender_male  class_2nd  class_3rd  \\\n",
       "0  42.0   7.11      0      0         True      False       True   \n",
       "1  13.0  20.05      0      2         True      False       True   \n",
       "2  16.0  20.05      1      1         True      False       True   \n",
       "3  39.0  20.05      1      1        False      False       True   \n",
       "4  16.0   7.13      0      0        False      False       True   \n",
       "\n",
       "   class_deck crew  class_engineering crew  class_restaurant staff  \\\n",
       "0            False                   False                   False   \n",
       "1            False                   False                   False   \n",
       "2            False                   False                   False   \n",
       "3            False                   False                   False   \n",
       "4            False                   False                   False   \n",
       "\n",
       "   class_victualling crew  embarked_Cherbourg  embarked_Queenstown  \\\n",
       "0                   False               False                False   \n",
       "1                   False               False                False   \n",
       "2                   False               False                False   \n",
       "3                   False               False                False   \n",
       "4                   False               False                False   \n",
       "\n",
       "   embarked_Southampton  \n",
       "0                  True  \n",
       "1                  True  \n",
       "2                  True  \n",
       "3                  True  \n",
       "4                  True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_transformed = pd.get_dummies(X, drop_first=True)\n",
    "display(X.head())\n",
    "display(X_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_hut = sp.symbols('y y_hut')\n",
    "lambda_reg = sp.symbols('lambda_reg')\n",
    "f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "f = (y_hut-y)**2 + lambda_reg*(y_hut**2)\n",
    "\n",
    "grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "y_pred = dtrain.get_label()\n",
    "y_rel = dtrain.get_label()\n",
    "\n",
    "lambda_r = 0.1\n",
    "grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "if type(grad) == float:\n",
    "    grad = np.ones(y_pred.shape[0]) * grad\n",
    "\n",
    "if type(hess) == float:\n",
    "    hess = np.ones(y_pred.shape[0]) * hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup\n",
    "\n",
    "def XGBoost_changed(lambda_r: float, dtrain: xgb.DMatrix) -> xgb.Booster:\n",
    "    def objective_function(y_pred: np.ndarray, dtrain_: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        y, y_hut = sp.symbols('y y_hut', real=True)\n",
    "        lambda_reg = sp.symbols('lambda_reg')\n",
    "        f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "        f = (y_hut-y)**2 + lambda_reg*y_hut\n",
    "\n",
    "        grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "        hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "        y_rel = dtrain_.get_label()\n",
    "\n",
    "        grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "        if type(grad) in [float, int]:\n",
    "            grad = np.ones(y_pred.shape[0]) * grad\n",
    "        if type(hess) in [float, int]:\n",
    "            hess = np.ones(y_pred.shape[0]) * hess\n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "    params = {\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': 2001,\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=8,\n",
    "        obj=objective_function,\n",
    "        evals=[(dtrain, 'train')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "list = []\n",
    "def XGBoost_changed(lambda_r: float, dtrain: xgb.DMatrix) -> xgb.Booster:\n",
    "    def objective_function(y_pred: np.ndarray, dtrain_: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        w = np.matmul(y_pred, np.linalg.pinv(dtrain.get_data().toarray()).T)\n",
    "        print(sum(w))\n",
    "        y, y_hut = sp.symbols('y y_hut', real=True)\n",
    "        lambda_reg = sp.symbols('lambda_reg')\n",
    "        f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "        f = (y_hut-y)**2 + lambda_reg * np.sum(w**2)\n",
    "\n",
    "        grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "        hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "        y_rel = dtrain_.get_label()\n",
    "        \n",
    "        list.append(w)\n",
    "\n",
    "        grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "        if type(grad) in [float, int]:\n",
    "            grad = np.ones(y_pred.shape[0]) * grad\n",
    "        if type(hess) in [float, int]:\n",
    "            hess = np.ones(y_pred.shape[0]) * hess\n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "    params = {\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': 2001,\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=8,\n",
    "        obj=objective_function,\n",
    "        # evals=[(dtrain, 'train')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n",
      "1.8405742340255529\n",
      "2.0289215915836394\n",
      "2.106331042246893\n",
      "2.2116179736331105\n",
      "2.3271447368897498\n",
      "2.3900851011276245\n",
      "2.432614288525656\n",
      "2.4517634578514844\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(0, 2, 0.05)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for lambda_r in arr:\n",
    "    model = XGBoost_changed(float(lambda_r), dtrain)\n",
    "    set = dtest\n",
    "    pred = model.predict(set)\n",
    "    df = pd.concat([df, pd.DataFrame({'lambda_reg': lambda_r, \n",
    "                  'test': balanced_accuracy_score(dtest.get_label(), model.predict(dtest) > 0.5),\n",
    "                  'train': balanced_accuracy_score(dtrain.get_label(), model.predict(dtrain) > 0.5)},\n",
    "                  index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.052108</td>\n",
       "      <td>0.180707</td>\n",
       "      <td>0.186006</td>\n",
       "      <td>0.223946</td>\n",
       "      <td>0.202835</td>\n",
       "      <td>0.201657</td>\n",
       "      <td>0.222810</td>\n",
       "      <td>0.212235</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.154881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>-0.089109</td>\n",
       "      <td>0.190585</td>\n",
       "      <td>0.148625</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.173919</td>\n",
       "      <td>0.235656</td>\n",
       "      <td>0.294631</td>\n",
       "      <td>0.248031</td>\n",
       "      <td>0.212898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>-0.177515</td>\n",
       "      <td>0.195027</td>\n",
       "      <td>0.124832</td>\n",
       "      <td>0.475646</td>\n",
       "      <td>0.205775</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.229518</td>\n",
       "      <td>0.361184</td>\n",
       "      <td>0.306459</td>\n",
       "      <td>0.254592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>-0.242689</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0.110424</td>\n",
       "      <td>0.552828</td>\n",
       "      <td>0.218683</td>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.229668</td>\n",
       "      <td>0.404047</td>\n",
       "      <td>0.345785</td>\n",
       "      <td>0.278796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>-0.009546</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>-0.292888</td>\n",
       "      <td>0.210279</td>\n",
       "      <td>0.113465</td>\n",
       "      <td>0.616172</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>0.120754</td>\n",
       "      <td>0.245539</td>\n",
       "      <td>0.428095</td>\n",
       "      <td>0.362031</td>\n",
       "      <td>0.293132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>-0.242689</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0.110424</td>\n",
       "      <td>0.552828</td>\n",
       "      <td>0.218683</td>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.229668</td>\n",
       "      <td>0.404047</td>\n",
       "      <td>0.345785</td>\n",
       "      <td>0.278796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>-0.009546</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>-0.292888</td>\n",
       "      <td>0.210279</td>\n",
       "      <td>0.113465</td>\n",
       "      <td>0.616172</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>0.120754</td>\n",
       "      <td>0.245539</td>\n",
       "      <td>0.428095</td>\n",
       "      <td>0.362031</td>\n",
       "      <td>0.293132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>-0.010372</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.325452</td>\n",
       "      <td>0.221569</td>\n",
       "      <td>0.105698</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>0.241484</td>\n",
       "      <td>0.097165</td>\n",
       "      <td>0.256951</td>\n",
       "      <td>0.463322</td>\n",
       "      <td>0.373867</td>\n",
       "      <td>0.302949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>-0.352692</td>\n",
       "      <td>0.218118</td>\n",
       "      <td>0.100481</td>\n",
       "      <td>0.670176</td>\n",
       "      <td>0.251384</td>\n",
       "      <td>0.105104</td>\n",
       "      <td>0.267901</td>\n",
       "      <td>0.480753</td>\n",
       "      <td>0.385730</td>\n",
       "      <td>0.314055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>-0.016542</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.368551</td>\n",
       "      <td>0.223363</td>\n",
       "      <td>0.097922</td>\n",
       "      <td>0.694548</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.083376</td>\n",
       "      <td>0.271015</td>\n",
       "      <td>0.499689</td>\n",
       "      <td>0.388118</td>\n",
       "      <td>0.320647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.003569  0.000889  0.012174  0.001958  0.052108  0.180707  0.186006   \n",
       "1    0.003396  0.001220  0.010177  0.003021 -0.089109  0.190585  0.148625   \n",
       "2    0.003215  0.001474  0.000042  0.003008 -0.177515  0.195027  0.124832   \n",
       "3    0.003072  0.001677 -0.003462  0.003437 -0.242689  0.194703  0.110424   \n",
       "4    0.002864  0.001867 -0.009546  0.003306 -0.292888  0.210279  0.113465   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "315  0.003072  0.001677 -0.003462  0.003437 -0.242689  0.194703  0.110424   \n",
       "316  0.002864  0.001867 -0.009546  0.003306 -0.292888  0.210279  0.113465   \n",
       "317  0.002724  0.001972 -0.010372  0.000027 -0.325452  0.221569  0.105698   \n",
       "318  0.002681  0.002024 -0.016069  0.002967 -0.352692  0.218118  0.100481   \n",
       "319  0.002607  0.002066 -0.016542  0.000062 -0.368551  0.223363  0.097922   \n",
       "\n",
       "           7         8         9         10        11        12        13  \n",
       "0    0.223946  0.202835  0.201657  0.222810  0.212235  0.184800  0.154881  \n",
       "1    0.377471  0.218400  0.173919  0.235656  0.294631  0.248031  0.212898  \n",
       "2    0.475646  0.205775  0.123075  0.229518  0.361184  0.306459  0.254592  \n",
       "3    0.552828  0.218683  0.114650  0.229668  0.404047  0.345785  0.278796  \n",
       "4    0.616172  0.232073  0.120754  0.245539  0.428095  0.362031  0.293132  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "315  0.552828  0.218683  0.114650  0.229668  0.404047  0.345785  0.278796  \n",
       "316  0.616172  0.232073  0.120754  0.245539  0.428095  0.362031  0.293132  \n",
       "317  0.658180  0.241484  0.097165  0.256951  0.463322  0.373867  0.302949  \n",
       "318  0.670176  0.251384  0.105104  0.267901  0.480753  0.385730  0.314055  \n",
       "319  0.694548  0.253442  0.083376  0.271015  0.499689  0.388118  0.320647  \n",
       "\n",
       "[320 rows x 14 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Regularization impact on accuracy'}, xlabel='lambda_reg'>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x='lambda_reg', y=['test', 'train'], title='Regularization impact on accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m     23\u001b[0m y_rel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m wT\u001b[38;5;241m*\u001b[39mxi\n\u001b[0;32m     26\u001b[0m w \u001b[38;5;241m=\u001b[39m xiT\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39my_pred\n\u001b[0;32m     29\u001b[0m y_pred \u001b[38;5;241m-\u001b[39m y_rel\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wT' is not defined"
     ]
    }
   ],
   "source": [
    "y, y_hut = sp.symbols('y y_hut', real=True)\n",
    "lambda_reg = sp.symbols('lambda_reg')\n",
    "f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "f = (y_hut-y)**2 + lambda_reg*abs(y_hut)\n",
    "\n",
    "grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "y_rel = dtrain.get_label()\n",
    "\n",
    "# grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "# if type(grad) in [float, int]:\n",
    "#     grad = np.ones(y_pred.shape[0]) * grad\n",
    "# if type(hess) in [float, int]:\n",
    "#     hess = np.ones(y_pred.shape[0]) * hess\n",
    "        \n",
    "# grad, hess\n",
    "\n",
    "# f.diff(y_hut)\n",
    "y_pred = np.linspace(0, 1, 15)\n",
    "y_rel = np.linspace(0, 10, 15)\n",
    "\n",
    "y_pred = wT*xi\n",
    "w = xiT-1*y_pred\n",
    "\n",
    "\n",
    "y_pred - y_rel\n",
    "\n",
    "\n",
    "    \n",
    "    # params = {\n",
    "    #     # 'objective': 'reg:squarederror',\n",
    "    #     'tree_method': 'hist',\n",
    "    #     'seed': 2001,\n",
    "    # }\n",
    "\n",
    "    # model = xgb.train(\n",
    "    #     params=params,\n",
    "    #     dtrain=dtrain,\n",
    "    #     num_boost_round=100,\n",
    "    #     obj=objective_function\n",
    "    # )\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5160378499108447"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dtrain.get_label()\n",
    "X = dtrain.get_data()\n",
    "\n",
    "w = np.matmul(y, np.linalg.pinv(dtrain.get_data().toarray()).T)\n",
    "\n",
    "sum(w**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
