{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dalex as dx\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = dx.datasets.load_titanic()\n",
    "X = titanic.drop(columns='survived')\n",
    "y = titanic.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "      <th>embarked</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age class     embarked   fare  sibsp  parch\n",
       "0    male  42.0   3rd  Southampton   7.11      0      0\n",
       "1    male  13.0   3rd  Southampton  20.05      0      2\n",
       "2    male  16.0   3rd  Southampton  20.05      1      1\n",
       "3  female  39.0   3rd  Southampton  20.05      1      1\n",
       "4  female  16.0   3rd  Southampton   7.13      0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>class_2nd</th>\n",
       "      <th>class_3rd</th>\n",
       "      <th>class_deck crew</th>\n",
       "      <th>class_engineering crew</th>\n",
       "      <th>class_restaurant staff</th>\n",
       "      <th>class_victualling crew</th>\n",
       "      <th>embarked_Cherbourg</th>\n",
       "      <th>embarked_Queenstown</th>\n",
       "      <th>embarked_Southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>20.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   fare  sibsp  parch  gender_male  class_2nd  class_3rd  \\\n",
       "0  42.0   7.11      0      0         True      False       True   \n",
       "1  13.0  20.05      0      2         True      False       True   \n",
       "2  16.0  20.05      1      1         True      False       True   \n",
       "3  39.0  20.05      1      1        False      False       True   \n",
       "4  16.0   7.13      0      0        False      False       True   \n",
       "\n",
       "   class_deck crew  class_engineering crew  class_restaurant staff  \\\n",
       "0            False                   False                   False   \n",
       "1            False                   False                   False   \n",
       "2            False                   False                   False   \n",
       "3            False                   False                   False   \n",
       "4            False                   False                   False   \n",
       "\n",
       "   class_victualling crew  embarked_Cherbourg  embarked_Queenstown  \\\n",
       "0                   False               False                False   \n",
       "1                   False               False                False   \n",
       "2                   False               False                False   \n",
       "3                   False               False                False   \n",
       "4                   False               False                False   \n",
       "\n",
       "   embarked_Southampton  \n",
       "0                  True  \n",
       "1                  True  \n",
       "2                  True  \n",
       "3                  True  \n",
       "4                  True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_transformed = pd.get_dummies(X, drop_first=True)\n",
    "display(X.head())\n",
    "display(X_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1765,), (1765,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y_hut = sp.symbols('y y_hut')\n",
    "lambda_reg = sp.symbols('lambda_reg')\n",
    "f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "f = (y_hut-y)**2 + lambda_reg*(y_hut**2)\n",
    "\n",
    "grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "y_pred = dtrain.get_label()\n",
    "y_rel = dtrain.get_label()\n",
    "\n",
    "lambda_r = 0.1\n",
    "grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "if type(grad) == float:\n",
    "    grad = np.ones(y_pred.shape[0]) * grad\n",
    "\n",
    "if type(hess) == float:\n",
    "    hess = np.ones(y_pred.shape[0]) * hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_changed(lambda_r: float, dtrain: xgb.DMatrix) -> xgb.Booster:\n",
    "    def objective_function(y_pred: np.ndarray, dtrain_: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        y, y_hut = sp.symbols('y y_hut')\n",
    "        lambda_reg = sp.symbols('lambda_reg')\n",
    "        f = sp.Function('f')(y, y_hut, lambda_reg)\n",
    "\n",
    "        f = (y_hut-y)**4 + lambda_reg*(y_hut**2)\n",
    "\n",
    "        grad = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut), 'numpy')\n",
    "        hess = sp.lambdify((y, y_hut, lambda_reg), f.diff(y_hut, y_hut), 'numpy')\n",
    "\n",
    "        y_rel = dtrain_.get_label()\n",
    "\n",
    "        grad, hess = grad(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r), hess(y=y_rel, y_hut=y_pred, lambda_reg=lambda_r)\n",
    "\n",
    "        if type(grad) in [float, int]:\n",
    "            grad = np.ones(y_pred.shape[0]) * grad\n",
    "        if type(hess) in [float, int]:\n",
    "            hess = np.ones(y_pred.shape[0]) * hess\n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "    params = {\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': 2001,\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=100,\n",
    "        obj=objective_function\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla lambda 0.001 to:  0.7530272692601068\n",
      "Dla lambda 0.002030917620904735 to:  0.7345776125095347\n",
      "Dla lambda 0.004124626382901352 to:  0.723064454614798\n",
      "Dla lambda 0.008376776400682925 to:  0.7349113272311212\n",
      "Dla lambda 0.017012542798525893 to:  0.7164616704805492\n",
      "Dla lambda 0.0345510729459222 to:  0.7315980167810832\n",
      "Dla lambda 0.07017038286703829 to:  0.7227069031273836\n",
      "Dla lambda 0.14251026703029993 to:  0.6851401601830663\n",
      "Dla lambda 0.28942661247167517 to:  0.6426392067124332\n",
      "Dla lambda 0.5878016072274912 to:  0.5615942028985508\n",
      "Dla lambda 1.1937766417144369 to:  0.5072463768115942\n",
      "Dla lambda 2.424462017082331 to:  0.5\n",
      "Dla lambda 4.923882631706742 to:  0.5\n",
      "Dla lambda 10.0 to:  0.5\n"
     ]
    }
   ],
   "source": [
    "for lambda_r in np.logspace(-3, 1, 14):\n",
    "    model = XGBoost_changed(float(lambda_r), dtrain)\n",
    "    set = dtest\n",
    "    pred = model.predict(set)\n",
    "    print(\"Dla lambda\", lambda_r, \"to: \", balanced_accuracy_score(set.get_label(), pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'lambda_reg': np.arange(0, 2, 0.025)})\n",
    "df['test'] = df['lambda_reg'].apply(lambda x: score(x, dtest))\n",
    "df['train'] = df['lambda_reg'].apply(lambda x: score(x, dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='lambda_reg', y=['test', 'train'], title='Regularization impact on accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_train, preds > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n",
    "\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective=squared_log)\n",
    "                    # 'binary:logistic')\n",
    "\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "preds = bst.predict(X_test)\n",
    "\n",
    "balanced_accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "num_classes = 3\n",
    "X, y = make_classification(n_samples=1000, n_informative=5,\n",
    "                           n_classes=num_classes)\n",
    "dtrain = xgb.DMatrix(data=X, label=y)\n",
    "num_parallel_tree = 4\n",
    "num_boost_round = 16\n",
    "# total number of built trees is num_parallel_tree * num_classes * num_boost_round\n",
    "\n",
    "# We build a boosted random forest for classification here.\n",
    "booster = xgb.train({\n",
    "    'num_parallel_tree': 4, 'subsample': 0.5, 'num_class': 3},\n",
    "                    num_boost_round=num_boost_round, dtrain=dtrain)\n",
    "\n",
    "# This is the sliced model, containing [3, 7) forests\n",
    "# step is also supported with some limitations like negative step is invalid.\n",
    "sliced: xgb.Booster = booster[3:7]\n",
    "\n",
    "# Access individual tree layer\n",
    "trees = [_ for _ in booster]\n",
    "assert len(trees) == num_boost_round\n",
    "\n",
    "print(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import Tuple\n",
    "\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n",
    "            np.power(predt + 1, 2))\n",
    "\n",
    "def squared_log(predt: np.ndarray,\n",
    "                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as\n",
    "    objective function.\n",
    "    '''\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Zakodowanie kolumn i zbudowanie pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['age', 'fare', 'sibsp', 'parch']),\n",
    "        ('cat', OneHotEncoder(), ['gender', 'class', 'embarked'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(objective='binary:logistic'))\n",
    "])\n",
    "\n",
    "# Dopasowanie modelu\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prognozowanie\n",
    "preds = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.DataFrame(y)\n",
    "dtrain = xgb.DMatrix(X_transformed, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import Tuple\n",
    "\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n",
    "            np.power(predt + 1, 2))\n",
    "\n",
    "def squared_log(predt: np.ndarray,\n",
    "                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as\n",
    "    objective function.\n",
    "    '''\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(objective=squared_log)\n",
    "xgbc.fit(X_train, y_train)\n",
    "\n",
    "# print(xgbc.score(X_test, y_test))\n",
    "# print(xgbc.score(X_test, y_test))\n",
    "# print(xgbc.score(X_train, y_train))\n",
    "\n",
    "xgbc.train({'learning_rate': 0.1, 'max_depth': 2, 'objective': 'binary:logistic'}, dtrain, num_boost_round=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_log = xgb.train({'tree_method': 'hist', 'seed': 1994},  # any other tree method is fine.\n",
    "           dtrain=dtrain,\n",
    "           num_boost_round=100\n",
    "           )\n",
    "\n",
    "xgbc = xgb.XGBClassifier(booster=squared_log)\n",
    "xgbc.fit(X_train, y_train)\n",
    "# print(xgbc.score(X_test, y_test))\n",
    "# print(xgbc.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softprob_obj(labels: np.ndarray, predt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rows = labels.shape[0]\n",
    "    classes = predt.shape[1]\n",
    "    grad = np.zeros((rows, classes), dtype=float)\n",
    "    hess = np.zeros((rows, classes), dtype=float)\n",
    "    eps = 1e-6\n",
    "    for r in range(predt.shape[0]):\n",
    "        target = labels[r]\n",
    "        p = softmax(predt[r, :])\n",
    "        for c in range(predt.shape[1]):\n",
    "            g = p[c] - 1.0 if c == target else p[c]\n",
    "            h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)\n",
    "            grad[r, c] = g\n",
    "            hess[r, c] = h\n",
    "\n",
    "    grad = grad.reshape((rows * classes, 1))\n",
    "    hess = hess.reshape((rows * classes, 1))\n",
    "    return grad, hess\n",
    "\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", objective=softprob_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=mean_absolute_error,\n",
    "    objective=softprob_obj\n",
    ")\n",
    "reg.fit(X, y, eval_set=[(X, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(objective=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = list(X.dtypes[(X.dtypes != 'object') & (X.dtypes != 'category')].index)\n",
    "categorical_features = list(X.dtypes[(X.dtypes == 'object') | (X.dtypes == 'category')].index)\n",
    "\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_transformer, categorical_features),\n",
    "    ('numerical', numerical_transformer, numerical_features)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "print(pipeline.score(X_test, y_test))\n",
    "print(pipeline.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(), ['age', 'fare', 'parch', 'sibsp']),\n",
    "    (OneHotEncoder(), ['gender', 'class', 'embarked']))\n",
    "\n",
    "titanic_rf = make_pipeline(\n",
    "    preprocess,\n",
    "    RandomForestClassifier(max_depth = 3, n_estimators = 500))\n",
    "titanic_rf.fit(X, y)\n",
    "\n",
    "titanic_lr = make_pipeline(\n",
    "    preprocess,\n",
    "    LogisticRegression(penalty = 'l2'))\n",
    "titanic_lr.fit(X, y)\n",
    "\n",
    "titanic_rf = make_pipeline(\n",
    "    preprocess,\n",
    "    RandomForestClassifier(max_depth = 3, n_estimators = 500))\n",
    "titanic_rf.fit(X, y)\n",
    "\n",
    "titanic_gbc = make_pipeline(\n",
    "    preprocess,\n",
    "    GradientBoostingClassifier(n_estimators = 100))\n",
    "titanic_gbc.fit(X, y)\n",
    "\n",
    "\n",
    "titanic_svm = make_pipeline(\n",
    "    preprocess,\n",
    "    SVC(probability = True))\n",
    "titanic_svm.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "henry = pd.DataFrame({'gender'   : ['male'],\n",
    "                       'age'     : [47],\n",
    "                       'class'   : ['1st'],\n",
    "                       'embarked': ['Cherbourg'],\n",
    "                       'fare'    : [25],\n",
    "                       'sibsp'   : [0],\n",
    "                       'parch'   : [0]},\n",
    "                      index = ['Henry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import Tuple\n",
    "\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n",
    "            np.power(predt + 1, 2))\n",
    "\n",
    "def squared_log(predt: np.ndarray,\n",
    "                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as\n",
    "    objective function.\n",
    "    '''\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(objective=squared_log, dtrain=dtrain)\n",
    "# xgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "y, y_pred = sp.symbols('y y_pred')\n",
    "f = sp.Function('f')(y, y_pred)\n",
    "\n",
    "# Create a loss function\n",
    "f = (y - y_pred)**2\n",
    "\n",
    "grad = f.gra\n",
    "# hess = grad.diff(y_pred)\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Function, hessian, pprint\n",
    "from sympy.abc import x, y\n",
    "f = Function('f')(x, y)\n",
    "g1 = Function('g')(x, y)\n",
    "g2 = x**2 + 3*y + 6*x**23 * y**3\n",
    "\n",
    "pprint(hessian(g2, (x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import Tuple\n",
    "\n",
    "def gradient_mse(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient mean squared error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return 2 * (predt - y)\n",
    "\n",
    "def hessian_mse(predt: np.ndarray, dtrain: xgb.DMatrix, lambda_reg: int) -> np.ndarray:\n",
    "    '''Compute the hessian for mean squared error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return 2 * np.ones_like(y) + lambda_reg\n",
    "\n",
    "def mean_squared_error(predt: np.ndarray,\n",
    "                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Mean squared error objective'''\n",
    "    grad = gradient_mse(predt, dtrain)\n",
    "    hess = hessian_mse(predt, dtrain, 1000)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.DataFrame(y)\n",
    "dtrain = xgb.DMatrix(X_transformed, label=label)\n",
    "\n",
    "gradient_mse(predt = y, dtrain = dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    ''' Root mean squared log error metric.'''\n",
    "    y = dtrain.get_label()\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    elements = np.power(np.log1p(y) - np.log1p(predt), 2)\n",
    "    return 'MSE', float(np.sqrt(np.sum(elements) / len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.train({'tree_method': 'hist', 'seed': 1994,\n",
    "           'gamma': 1},\n",
    "           dtrain=dtrain,\n",
    "           num_boost_round=20,\n",
    "           obj=mean_squared_error,\n",
    "        #    custom_metric=rmsle,\n",
    "           evals=[(dtrain, 'train'), (dtrain, 'test')]\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import loggamma, psi as digamma, polygamma\n",
    "trigamma = lambda x: polygamma(1, x)\n",
    "\n",
    "def dirichlet_fun(pred: np.ndarray, Y: np.ndarray) -> float:\n",
    "    epred = np.exp(pred)\n",
    "    sum_epred = np.sum(epred, axis=1, keepdims=True)\n",
    "    return (\n",
    "        loggamma(epred).sum()\n",
    "        - loggamma(sum_epred).sum()\n",
    "        - np.sum(np.log(Y) * (epred - 1))\n",
    "    )\n",
    "def dirichlet_grad(pred: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    epred = np.exp(pred)\n",
    "    return epred * (\n",
    "        digamma(epred)\n",
    "        - digamma(np.sum(epred, axis=1, keepdims=True))\n",
    "        - np.log(Y)\n",
    "    )\n",
    "def dirichlet_hess(pred: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    epred = np.exp(pred)\n",
    "    grad = dirichlet_grad(pred, Y)\n",
    "    k = Y.shape[1]\n",
    "    H = np.empty((pred.shape[0], k, k))\n",
    "    for row in range(pred.shape[0]):\n",
    "        H[row, :, :] = (\n",
    "            - trigamma(epred[row].sum()) * np.outer(epred[row], epred[row])\n",
    "            + np.diag(grad[row] + trigamma(epred[row]) * epred[row] ** 2)\n",
    "        )\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "from scipy import stats\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import softmax\n",
    "\n",
    "def gen_random_dirichlet(rng: np.random.Generator, m: int, k: int):\n",
    "    alpha = np.exp(rng.standard_normal(size=k))\n",
    "    return rng.dirichlet(alpha, size=m)\n",
    "\n",
    "def test_dirichlet_fun_grad_hess():\n",
    "    k = 3\n",
    "    m = 10\n",
    "    rng = np.random.default_rng(seed=123)\n",
    "    Y = gen_random_dirichlet(rng, m, k)\n",
    "    x0 = rng.standard_normal(size=k)\n",
    "    for row in range(Y.shape[0]):\n",
    "        fun_row = dirichlet_fun(x0.reshape((1,-1)), Y[[row]])\n",
    "        ref_logpdf = stats.dirichlet.logpdf(\n",
    "            Y[row] / Y[row].sum(), # <- avoid roundoff error\n",
    "            np.exp(x0),\n",
    "        )\n",
    "        assert isclose(fun_row, -ref_logpdf)\n",
    "\n",
    "        gdiff = check_grad(\n",
    "            lambda pred: dirichlet_fun(pred.reshape((1,-1)), Y[[row]]),\n",
    "            lambda pred: dirichlet_grad(pred.reshape((1,-1)), Y[[row]]),\n",
    "            x0\n",
    "        )\n",
    "        assert gdiff <= 1e-6\n",
    "\n",
    "        H_numeric = np.empty((k,k))\n",
    "        eps = 1e-7\n",
    "        for ii in range(k):\n",
    "            x0_plus_eps = x0.reshape((1,-1)).copy()\n",
    "            x0_plus_eps[0,ii] += eps\n",
    "            for jj in range(k):\n",
    "                H_numeric[ii, jj] = (\n",
    "                    dirichlet_grad(x0_plus_eps, Y[[row]])[0][jj]\n",
    "                    - dirichlet_grad(x0.reshape((1,-1)), Y[[row]])[0][jj]\n",
    "                ) / eps\n",
    "        H = dirichlet_hess(x0.reshape((1,-1)), Y[[row]])[0]\n",
    "        np.testing.assert_almost_equal(H, H_numeric, decimal=6)\n",
    "test_dirichlet_fun_grad_hess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_expected_hess(pred: np.ndarray) -> np.ndarray:\n",
    "    epred = np.exp(pred)\n",
    "    k = pred.shape[1]\n",
    "    Ehess = np.empty((pred.shape[0], k, k))\n",
    "    for row in range(pred.shape[0]):\n",
    "        Ehess[row, :, :] = (\n",
    "            - trigamma(epred[row].sum()) * np.outer(epred[row], epred[row])\n",
    "            + np.diag(trigamma(epred[row]) * epred[row] ** 2)\n",
    "        )\n",
    "    return Ehess\n",
    "def test_dirichlet_expected_hess():\n",
    "    k = 3\n",
    "    rng = np.random.default_rng(seed=123)\n",
    "    x0 = rng.standard_normal(size=k)\n",
    "    y_sample = rng.dirichlet(np.exp(x0), size=5_000_000)\n",
    "    x_broadcast = np.broadcast_to(x0, (y_sample.shape[0], k))\n",
    "    g_sample = dirichlet_grad(x_broadcast, y_sample)\n",
    "    ref = (g_sample.T @ g_sample) / y_sample.shape[0]\n",
    "    Ehess = dirichlet_expected_hess(x0.reshape((1,-1)))[0]\n",
    "    np.testing.assert_almost_equal(Ehess, ref, decimal=2)\n",
    "test_dirichlet_expected_hess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_diag_upper_bound_expected_hess(\n",
    "    pred: np.ndarray, Y: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    Ehess = dirichlet_expected_hess(pred)\n",
    "    diag_bound_Ehess = np.empty((pred.shape[0], Y.shape[1]))\n",
    "    for row in range(pred.shape[0]):\n",
    "        diag_bound_Ehess[row, :] = np.abs(Ehess[row, :, :]).sum(axis=1)\n",
    "    return diag_bound_Ehess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from typing import Tuple\n",
    "\n",
    "def dirichlet_xgb_objective(\n",
    "    pred: np.ndarray, dtrain: xgb.DMatrix\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Y = dtrain.get_label().reshape(pred.shape)\n",
    "    return (\n",
    "        dirichlet_grad(pred, Y),\n",
    "        dirichlet_diag_upper_bound_expected_hess(pred, Y),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_eval_metric(\n",
    "    pred: np.ndarray, dtrain: xgb.DMatrix\n",
    ") -> Tuple[str, float]:\n",
    "    Y = dtrain.get_label().reshape(pred.shape)\n",
    "    return \"dirichlet_ll\", dirichlet_fun(pred, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    10.4,11.7,12.8,13,15.7,16.3,18,18.7,20.7,22.1,\n",
    "    22.4,24.4,25.8,32.5,33.6,36.8,37.8,36.9,42.2,47,\n",
    "    47.1,48.4,49.4,49.5,59.2,60.1,61.7,62.4,69.3,73.6,\n",
    "    74.4,78.5,82.9,87.7,88.1,90.4,90.6,97.7,103.7,\n",
    "]).reshape((-1,1))\n",
    "# sand, silt, clay\n",
    "Y = np.array([\n",
    "    [0.775,0.195,0.03], [0.719,0.249,0.032], [0.507,0.361,0.132],\n",
    "    [0.522,0.409,0.066], [0.7,0.265,0.035], [0.665,0.322,0.013],\n",
    "    [0.431,0.553,0.016], [0.534,0.368,0.098], [0.155,0.544,0.301],\n",
    "    [0.317,0.415,0.268], [0.657,0.278,0.065], [0.704,0.29,0.006],\n",
    "    [0.174,0.536,0.29], [0.106,0.698,0.196], [0.382,0.431,0.187],\n",
    "    [0.108,0.527,0.365], [0.184,0.507,0.309], [0.046,0.474,0.48],\n",
    "    [0.156,0.504,0.34], [0.319,0.451,0.23], [0.095,0.535,0.37],\n",
    "    [0.171,0.48,0.349], [0.105,0.554,0.341], [0.048,0.547,0.41],\n",
    "    [0.026,0.452,0.522], [0.114,0.527,0.359], [0.067,0.469,0.464],\n",
    "    [0.069,0.497,0.434], [0.04,0.449,0.511], [0.074,0.516,0.409],\n",
    "    [0.048,0.495,0.457], [0.045,0.485,0.47], [0.066,0.521,0.413],\n",
    "    [0.067,0.473,0.459], [0.074,0.456,0.469], [0.06,0.489,0.451],\n",
    "    [0.063,0.538,0.399], [0.025,0.48,0.495], [0.02,0.478,0.502],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=Y)\n",
    "results: Dict[str, Dict[str, List[float]]] = {}\n",
    "booster = xgb.train(\n",
    "    params={\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"num_target\": Y.shape[1],\n",
    "        \"base_score\": 0,\n",
    "        \"disable_default_eval_metric\": True,\n",
    "        \"max_depth\": 3,\n",
    "        \"seed\": 123,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=10,\n",
    "    obj=dirichlet_xgb_objective,\n",
    "    evals=[(dtrain, \"Train\")],\n",
    "    evals_result=results,\n",
    "    custom_metric=dirichlet_eval_metric,\n",
    ")\n",
    "yhat = softmax(booster.inplace_predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def get_optimal_intercepts(Y: np.ndarray) -> np.ndarray:\n",
    "    k = Y.shape[1]\n",
    "    res = minimize(\n",
    "        fun=lambda pred: dirichlet_fun(\n",
    "            np.broadcast_to(pred, (Y.shape[0], k)),\n",
    "            Y\n",
    "        ),\n",
    "        x0=np.zeros(k),\n",
    "        jac=lambda pred: dirichlet_grad(\n",
    "            np.broadcast_to(pred, (Y.shape[0], k)),\n",
    "            Y\n",
    "        ).sum(axis=0)\n",
    "    )\n",
    "    return res[\"x\"]\n",
    "intercepts = get_optimal_intercepts(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_margin = np.broadcast_to(intercepts, Y.shape)\n",
    "dtrain_w_intercept = xgb.DMatrix(X, label=Y, base_margin=base_margin)\n",
    "results: Dict[str, Dict[str, List[float]]] = {}\n",
    "booster = xgb.train(\n",
    "    params={\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"num_target\": Y.shape[1],\n",
    "        \"base_score\": 0,\n",
    "        \"disable_default_eval_metric\": True,\n",
    "        \"max_depth\": 3,\n",
    "        \"seed\": 123,\n",
    "    },\n",
    "    dtrain=dtrain_w_intercept,\n",
    "    num_boost_round=10,\n",
    "    obj=dirichlet_xgb_objective,\n",
    "    evals=[(dtrain, \"Train\")],\n",
    "    evals_result=results,\n",
    "    custom_metric=dirichlet_eval_metric,\n",
    ")\n",
    "yhat = softmax(\n",
    "    booster.predict(\n",
    "        xgb.DMatrix(X, base_margin=base_margin)\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
